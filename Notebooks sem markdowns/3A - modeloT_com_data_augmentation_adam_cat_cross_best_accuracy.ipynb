{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6743fa08-2367-4cb7-b9a5-b123a4e34f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63eb630-54a1-4dd9-8f8a-7e0d62939f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a69de-6015-49b8-809f-2fe59d6f265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'dataset_balanceado_final/train'\n",
    "validation_dir = 'dataset_balanceado_final/validation'\n",
    "test_dir = 'dataset_balanceado_final/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb9856e-e795-44d6-9330-37da2f155b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "IMG_SIZE = 150\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78d735-15eb-420a-9b40-b5fb5d4f1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir, \n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45eb16-bc85-461d-968d-5db88e88efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = image_dataset_from_directory(\n",
    "    validation_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d01250-529d-4528-a3e0-b6d46003823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c08d43-c79f-433d-a237-9945e300bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza uma função(do sckicit-learn) para avaliar o desempenho do modelo, indicando Métricas como: \n",
    "    # f1-score do modelo\n",
    "    # accuracy do modelo\n",
    "    # accuracy por classe \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "def print_classification_metrics(model, dataset, phase_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        preds = model.predict(images)\n",
    "        y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "    print(f\"\\n {phase_name}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154bea2-1726-4d54-b4b4-0fabc6097f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from keras import layers \n",
    "from keras.applications import VGG16 # Importa a arquitetura VGG16 pré-treinada do Keras.\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation_vgg16 = keras.Sequential([ \n",
    "    layers.RandomFlip(\"vertical\"),# Aplica inversão vertical nas imagens.\n",
    "    layers.RandomFlip(\"horizontal\"), # Aplica inversão horizontal nas imagens\n",
    "    layers.RandomTranslation(0.1, 0.2), \n",
    "    layers.RandomRotation(0.4), # Aplica rotação de 20% \n",
    "    #layers.RandomZoom(0.1)\n",
    "    ]) # Aplica zoom de 20% \n",
    "\n",
    "# Carregar a base VGG16 pré-treinada\n",
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3)) # Carrega o modelo VGG16 pré-treinado no ImageNet, sem a camada do topo\n",
    "conv_base.trainable = False # Congela todas as camadas da VGG16, impedindo que os seus pesos sejam atualizados durante o treino (feature extraction).\n",
    "\n",
    "# Usar data augmentation\n",
    "inputs = layers.Input(shape=(150, 150, 3)) # Define a camada de entrada do novo modelo com o formato das imagens e 3 canais (RGB)\n",
    "x = data_augmentation_vgg16(inputs) # Aplica as transformações de aumento de dados nas imagens de entrada.\n",
    "x = keras.applications.vgg16.preprocess_input(x) # Aplica o pré-processamento específico da VGG16 \n",
    "x = conv_base(x) # Passa as imagens (pré-processadas e aumentadas) através da base VGG16 congelada para extrair características.\n",
    "x = layers.Flatten()(x) # Achata as características extraídas para um vetor 1D.\n",
    "x = layers.Dense(256, activation='relu')(x) # Adiciona uma camada densa com 256 neurónios e ativação ReLU.\n",
    "x = layers.Dropout(0.5)(x) # Aplica Dropout (50%) para regularização e prevenção de overfitting.\n",
    "outputs = layers.Dense(7, activation='softmax')(x) # Adiciona a camada de saída densa com 7 neurónios (para 7 classes) e ativação Softmax\n",
    "\n",
    "model_t = models.Model(inputs, outputs) # Cria o model_t\n",
    "\n",
    "# Compilar e treinar (feature extraction)\n",
    "model_t.compile( # Compila o modelo para configurar o processo de treino.\n",
    "    loss='categorical_crossentropy', # Define a função de loss categorical crossentropy.\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3), # Configura o otimizador Adam  learning_rate=1e-3) \n",
    "    metrics=['accuracy'] # Define 'accuracy' (precisão) como a métrica a ser monitorizada.\n",
    ")\n",
    "\n",
    "history_t = model_t.fit( # Treina o modelo.\n",
    "    train_dataset, # Usa o conjunto de dados de treino.\n",
    "    validation_data=validation_dataset, # Usa o conjunto de dados de validação para monitorizar o desempenho.\n",
    "    epochs=15 # Treina o modelo por 10 épocas.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa24eae-caa1-43ad-94bf-946221fedf35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_classification_metrics(model_t, test_dataset, \"Modelo 2 : VGG16 (Feature Extraction com Augmentation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cddb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history_t.history['accuracy']\n",
    "val_acc = history_t.history['val_accuracy']\n",
    "loss = history_t.history['loss']\n",
    "val_loss = history_t.history['val_loss']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9b598-ec41-44eb-b747-86d2c09226c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descongelar parte da VGG16 (últimas camadas)\n",
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-4]:  # Descongelar as ultimas -8\n",
    "    layer.trainable = False # manter as primeiras camadas congeladas\n",
    "\n",
    "# Recompilar o modelo com learning rate menor\n",
    "model_t.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Treinar novamente\n",
    "history_t = model_t.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=15 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80295bdb-b714-44a2-abe2-c9eff3ee92ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_classification_metrics(model_t, test_dataset, \"Modelo 2 : Fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b90a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history_t.history['accuracy']\n",
    "val_acc = history_t.history['val_accuracy']\n",
    "loss = history_t.history['loss']\n",
    "val_loss = history_t.history['val_loss']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61488591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obter predições no test_dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    preds = model_t.predict(images)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "class_names = test_dataset.class_names  \n",
    "\n",
    "# Criar e mostrar a matriz de confusão\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation=45, values_format='d')\n",
    "plt.title(\"Matriz de Confusão - Teste\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908670e3-ca9e-455c-8fbf-7d79891e17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.save(\"modelT_3A_com_data_aug_adam_cat_cross_best_acc.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3500594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def extract_computed_features(conv_base, dataset, augment_layer=None):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        # Aplica data augmentation se fornecido\n",
    "        if augment_layer:\n",
    "            images = augment_layer(images)\n",
    "        \n",
    "        # Pre-processamento obrigatório para VGG16\n",
    "        preprocessed = preprocess_input(images)\n",
    "        \n",
    "        # Extrair as computed features\n",
    "        features = conv_base.predict(preprocessed)\n",
    "        \n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Junta tudo num único array\n",
    "    features_array = np.concatenate(all_features)\n",
    "    labels_array = np.concatenate(all_labels)\n",
    "\n",
    "    return features_array, labels_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_computed_features(conv_base, train_dataset, augment_layer=data_augmentation_vgg16)\n",
    "val_features, val_labels = extract_computed_features(conv_base, validation_dataset, augment_layer=None)\n",
    "test_features, test_labels = extract_computed_features(conv_base, test_dataset, augment_layer=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "\n",
    "save(\"modelt_3A_train_features.npy\", train_features)\n",
    "save(\"modelt_3A_train_labels.npy\", train_labels)\n",
    "\n",
    "save(\"modelt_3A_val_features.npy\", val_features)\n",
    "save(\"modelt_3A_val_labels.npy\", val_labels)\n",
    "\n",
    "save(\"modelt_3A_test_features.npy\", test_features)\n",
    "save(\"modelt_3A_test_labels.npy\", test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
