{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6743fa08-2367-4cb7-b9a5-b123a4e34f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63eb630-54a1-4dd9-8f8a-7e0d62939f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a69de-6015-49b8-809f-2fe59d6f265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'dataset_balanceado_final/train'\n",
    "validation_dir = 'dataset_balanceado_final/validation'\n",
    "test_dir = 'dataset_balanceado_final/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb9856e-e795-44d6-9330-37da2f155b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "IMG_SIZE = 150\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78d735-15eb-420a-9b40-b5fb5d4f1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical' # categorical porque temos várias classes, senão seria binário (2 classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45eb16-bc85-461d-968d-5db88e88efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = image_dataset_from_directory(\n",
    "    validation_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d01250-529d-4528-a3e0-b6d46003823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452adb31-4a0d-4896-9f74-edf5ae795841",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([layers.RandomFlip(\"horizontal\"), # Efeito \"espelho\" a partir do eixo horizontal\n",
    "                                      layers.RandomFlip(\"vertical\"),# Aplica inversão vertical nas imagens.\n",
    "                                      layers.RandomTranslation(0.1, 0.2), \n",
    "                                      layers.RandomRotation(0.4),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c08d43-c79f-433d-a237-9945e300bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza uma função(do sckicit-learn) para avaliar o desempenho do modelo, indicando Métricas como: \n",
    "    # f1-score do modelo\n",
    "    # accuracy do modelo\n",
    "    # accuracy por classe \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "def print_classification_metrics(model, dataset, phase_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        preds = model.predict(images)\n",
    "        y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "    print(f\"\\n {phase_name}\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e49b20-4c59-4909-bb70-4238c5e7c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a camada de entrada do modelo com o formato das imagens (altura, largura, 3 canais RGB).\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ec47d-589f-4591-a0b1-5bcaa41dee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplica as transformações aleatórias às imagens (definidas no inicio do notebook) para aumentar a diversidade do dataset de treino e evitar overfitting.\n",
    "x = data_augmentation(inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9af0d-384a-41ba-8bf8-078ab2b933fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza os valores dos pixels das imagens de entrada para o intervalo [0, 1].\n",
    "x = layers.Rescaling(1./255)(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17106f6d-b9d5-498a-8c20-57ea5895c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeira camada convolucional com 64 filtros 3x3 e ativação ReLU\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\")(x) \n",
    "# adicionado o padding=\"same\" para garantir que o output da camada convolucional tem a mesma dimensão espacial (altura e largura) que o input, após a operação de convolução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45240d6-7932-4223-9775-6bf12037b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeira camada de pooling máximo (2x2) para reduzir a dimensionalidade.\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62253dc-3aa6-4575-8acb-bc169140390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segunda camada convolucional com 128 filtros 3x3 e ativação ReLU.\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n",
    "# Segunda camada de pooling máximo (2x2).\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a0ec4-0100-48c6-84d0-ddd046034b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terceira camada convolucional com 128 filtros 3x3 e ativação ReLU.\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n",
    "# Terceira camada de pooling máximo (2x2).\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021fcdd-1dc8-465c-a99e-4c206ff5c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achata o output das camadas convolucionais para um vetor 1D.\n",
    "x = layers.Flatten()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952beb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tirar comentario desta linha abaixo se queremos usar batch normalization\n",
    "# porem foi testado varias vezes e em diferentes camadas da rede, mas não melhorou o desempenho do modelo (e o optuna tambem sugeriu nao utilizar)\n",
    "#x = layers.BatchNormalization( axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,beta_initializer=\"zeros\",    moving_mean_initializer=\"zeros\", moving_variance_initializer=\"ones\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d79d4-34d0-42f4-a7d1-65dfbed76654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica Dropout (50%) para desativar aleatoriamente neurónios, prevenindo o overfitting.\n",
    "# Neste caso, para este modelo em especifico, atingiu a pior acc sem dropout\n",
    "#x = layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5f800-011f-4b53-96a4-fb63098ce4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir funçao de Regularização L2 (opcional)\n",
    "reg = regularizers.l2(0.01)  # Executar para ativar\n",
    "\n",
    "# Camada densa (totalmente conectada) com 128 neurónios, ativação ReLU e funçao de regularizaçao L2\n",
    "x = layers.Dense(128, activation=\"relu\", kernel_regularizer=reg)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48094fda-a84a-40ed-aac1-c55db903b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camada de saída densa com 7 neurónios e ativação Softmax (para classificação categórica).\n",
    "outputs = layers.Dense(7, activation=\"softmax\")(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55c088-528f-4988-ae7f-46082fa44fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o modelo Keras usando as camadas de entrada e saída definidas.\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708fb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o otimizador: SGD\n",
    "# Define a função de loss: MSE\n",
    "# Indica que a 'accuracy' (precisão) será a métrica durante o treino.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(),\n",
    "    loss='mse',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ecc518-2720-4fa5-ac9e-0b6029024b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(\n",
    "    #optimizer='SGD', \n",
    "    #loss='mse',\n",
    "    #metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0194b-06bb-484e-a116-3004a8b8cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(\n",
    "    #optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    #loss=tf.keras.losses.KLDivergence(),\n",
    "    #metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a7067-3e11-4d80-a1d9-606eb23f1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset, #Inicia o treino do modelo usando o conjunto de dados de treino.\n",
    "    epochs=25,     # O modelo será treinado por 25 épocas (passagens completas pelo conjunto de treino).\n",
    "    validation_data=validation_dataset) # Usa o conjunto de dados de validação para monitorizar o desempenho do modelo em dados não vistos durante o treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834d88a-789c-4f7d-a898-5c9fa4eb6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_classification_metrics(model, test_dataset, \"Modelo 1 : CNN de raiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obter predições no test_dataset\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    preds = model.predict(images)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "class_names = test_dataset.class_names  \n",
    "\n",
    "# Criar e mostrar a matriz de confusão\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation=45, values_format='d')\n",
    "plt.title(\"Matriz de Confusão - Teste\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b6fc6b-8055-4f3c-b062-dc111b741996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelS_2B3_com_data_aug_SGD_MSE_worst_acc.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
